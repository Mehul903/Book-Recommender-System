{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow of the code\n",
    "\n",
    "> **Part-1:** Define all the functions required to build a recommendation system\n",
    "\n",
    "> **Part-2:** Call (i.e. execute) those functions\n",
    "\n",
    "1. Load amazon-data and get unique book-names from entire file\n",
    "2. Load goodreads data and get unique book-names\n",
    "3. Find the book-names that exist in both the sets of book-names\n",
    "4. Do necessary cleaning and prepare data for exploration\n",
    "5. Build Item-by-Item CF and Matrix-Factorization-based recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "# %unload_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "## Import required modules:\n",
    "import pandas as pd\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1\n",
    "### Data Post-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "## Load several rows of data and see what we've got\n",
    "## Import reviews file (i.e. pre-processed amazon-book-reviews created by `reviews_preprocessing.ipynb` ):\n",
    "def load_data(path, cols):\n",
    "    \"\"\"\n",
    "    Load several rows of amazon-dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    books_amazon = pd.read_csv(path, nrows = 10, encoding = 'utf-8', names = cols)\n",
    "    \n",
    "    return books_amazon\n",
    "\n",
    "    \n",
    "## Get all the book-names from Amaozn-data (later this list will be used to find crime-mystery books):\n",
    "def get_unique_books(path):\n",
    "    \"\"\"\n",
    "    Get all the unique book names from the amazon-data-file. As the file is huge (~11 GB), I'll read entire file line-by-line \n",
    "    and extract book names and store them in the dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    book_names = {}\n",
    "    with open (path) as books:\n",
    "        for line in books:\n",
    "            book = line.split(',')[1].strip().lower()\n",
    "            if book not in book_names.keys():\n",
    "                book_names[book] = True    \n",
    "    \n",
    "    return book_names.keys()\n",
    "\n",
    "\n",
    "def load_goodreads_data(path):\n",
    "    \"\"\"\n",
    "    Load the data collected by scraping goodreads.com and clean the book-names.\n",
    "    \"\"\"\n",
    "\n",
    "    books_goodreads = pd.read_csv(path, usecols = ['Books'], encoding = 'latin1')\n",
    "    books_goodreads['Books'] = [book.strip().lower() for book in books_goodreads['Books']]\n",
    "    \n",
    "    # books_goodreads['Total_Num_Ratings'] = [int(rating.replace(',','')) for rating in books_goodreads['Total_Num_Ratings']]\n",
    "    # books_goodreads['Total_Num_Votes'] = [int(vote.replace(',','')) for vote in books_goodreads['Total_Num_Votes']]\n",
    "    # books_goodreads['Avg_Rating'] = [float(rating.strip(\"['']\")) for rating in books_goodreads['Avg_Rating']]\n",
    "    \n",
    "    return books_goodreads\n",
    "\n",
    "\n",
    "def get_matching_books(amazon_books, goodreads_books):\n",
    "    \"\"\"\n",
    "    Find the books-names that exist in both: amazon and goodreads data.\n",
    "    \"\"\"\n",
    "    \n",
    "    common_books = list(set(amazon_books) & set(goodreads_books))\n",
    "    common_book_dict = dict()\n",
    "    for i in common_books:\n",
    "        common_book_dict[i] = True\n",
    "    \n",
    "    return common_book_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start building a recommendation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "def subset_amazon_data(path, n = 1000000):\n",
    "    \"\"\"\n",
    "    Amaxon-data have ~14 million books ratings but here I am only interested in crime-mystery book-ratings. So, I'll subset \n",
    "    amazon-data for those books only. Moreover, I only use first n (here 1,000,000) rows to avoid memory-error issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_df = []\n",
    "    problematic_lines = []\n",
    "\n",
    "    rows = 0\n",
    "    with open (path) as books:\n",
    "        while rows < n:\n",
    "            line = books.readline()\n",
    "\n",
    "            try:\n",
    "                book = line.split(', ')[1].lower().strip()\n",
    "                if book in common_book_dict.keys():\n",
    "                    list_df.append(line)\n",
    "            except:\n",
    "                problematic_lines.append(line)\n",
    "                \n",
    "            rows+=1\n",
    "        \n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "def extract_info(data):\n",
    "    \"\"\"\n",
    "    Extract necessary information from list_df, namely book-ratings, books-ids, book-title, and user-ids, and save it as \n",
    "    dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get ratings\n",
    "    ratings = [float(re.search(pattern = ' [0-5][.]0',  string = i).group()) for i in data]\n",
    "\n",
    "    ## Get book IDs and names\n",
    "    book_ids = [i[0:11] for i in data]\n",
    "\n",
    "    ## Get book-names\n",
    "    book_titles = [i.split(', ')[1] for i in data]\n",
    "\n",
    "    ## Get user-ids\n",
    "    user_ids = [i.split(', ')[3][0:14] for i in data]\n",
    "    \n",
    "\n",
    "    books_amazon_whole = pd.DataFrame({'BookID': book_ids, 'BookTitle': book_titles, 'UserID': user_ids, 'Score': ratings})\n",
    "    books_amazon_whole = books_amazon_whole[books_amazon_whole['UserID'] != 'unknown']\n",
    "    books_amazon_whole = books_amazon_whole[~books_amazon_whole['UserID'].str.contains('[0-9]+[.][0-9]+')]\n",
    "\n",
    "    return books_amazon_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19 ms\n"
     ]
    }
   ],
   "source": [
    "def create_user_item_df(ratings_df):\n",
    "    \"\"\"\n",
    "    Convert books_amazon_whole dataframe into user-item ratings dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # books_amazon_whole.drop_duplicates(subset = ['BookTitle', 'UserID'], inplace = True)\n",
    "    user_item_df = pd.pivot_table(data = ratings_df, index = 'UserID', columns = 'BookTitle', values = 'Score')\n",
    "\n",
    "    ## Fill NAs with 0 and get the new list of book-titles:\n",
    "    user_item_df.fillna(0, inplace=True)\n",
    "\n",
    "    print ('Shape: ', user_item_df.shape)\n",
    "    \n",
    "    return user_item_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Item-by-item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "## Train-test split\n",
    "## Get #books rated by each user\n",
    "\n",
    "def generate_train_test_data(books_ratings, user_item_mat):\n",
    "    \"\"\"\n",
    "    Generate train/test set. In the training data, consider users who rated more than 2 books. And only those who rated more \n",
    "    than two books will only be a part of test-set because we'll predict the ratings for those additional books that they rated.\n",
    "    Whenever users rated >2 books, I randomly choose one of those ratings to be a part of test-data (which eventually be \n",
    "    predicted and be used to calculate error) and make specifically that rating '0' in the train-set.\n",
    "    \"\"\"\n",
    "    \n",
    "    count_ratings_per_user = Counter(books_ratings['UserID'])\n",
    "\n",
    "    ## list of users who rated more than 2 books\n",
    "    more_than_2 = [i[0] for i in count_ratings_per_user.items() if i[1] > 2]\n",
    "\n",
    "    test = np.zeros((len(more_than_2), user_item_mat.shape[1]))\n",
    "\n",
    "    for i,j in enumerate(more_than_2):\n",
    "        temp = books_ratings[books_ratings['UserID'] == j].groupby(by = 'UserID').agg(np.random.choice)\n",
    "\n",
    "        ## Populate test-matrix\n",
    "        ### Get column-index and rating to populate in test-data\n",
    "        col_id = user_item_mat.columns.get_loc(temp.iloc[0,1])\n",
    "        test[i, col_id] = temp.iloc[0,2]\n",
    "\n",
    "        ## Make this particular pair of user-boook rating 0 in train-data\n",
    "        user_item_mat.iloc[(user_item_mat.index == j), col_id] = 0\n",
    "        \n",
    "    return user_item_mat, test\n",
    "\n",
    "def calculate_similarity(user_item_ratings, e = 1e-5):\n",
    "    \"\"\"\n",
    "    Train model to get similarity-matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sim_mat = user_item_ratings.T.dot(user_item_ratings) + e\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim_mat))])\n",
    "        \n",
    "    return (sim_mat/norms/norms.T)\n",
    "\n",
    "\n",
    "def predict_ratings_train(user_item_ratings, similarity_mat):\n",
    "    \"\"\"\n",
    "    Predict ratings on training-data\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_ratings = user_item_ratings.dot(similarity_mat) / np.reshape(np.abs(similarity_mat).sum(axis = 1), (-1))\n",
    "    \n",
    "    return pred_ratings\n",
    "\n",
    "\n",
    "## Testing on test data\n",
    "def predict_ratings_test(test_data, similarity_mat):\n",
    "    \"\"\"\n",
    "    Preedict ratings on testing-data\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_test = test_data.dot(similarity_mat)/np.reshape(np.abs(similarity_mat).sum(axis = 1), (-1))    \n",
    "    \n",
    "    return pred_test\n",
    "\n",
    "\n",
    "def calculate_error(test_data, pred_data):\n",
    "    \"\"\"\n",
    "    Returns Root-Mean-Squared-Error (RMSE) for ratings predicted on test-set\n",
    "    \"\"\"\n",
    "\n",
    "    rmse = []\n",
    "    for i in range(test_data.shape[0]): \n",
    "        non_zero_ind = np.nonzero(test_data[i,:])\n",
    "        error = np.sqrt(np.sum((test_data[i, non_zero_ind] - np.array(pred_data[i, non_zero_ind])[0])**2))\n",
    "        rmse.append(error)\n",
    "\n",
    "    print ('Average RMSE: ')\n",
    "    \n",
    "    return round(np.mean(rmse), 2)\n",
    "\n",
    "\n",
    "## Make recommendations: can access actual user-ids from 'more_than_2' list using index from 'test'\n",
    "# print (user_item_df.columns[np.argsort(test[user_number,:])[::-1][0:3]])\n",
    "def get_recommendations(user_index, pred_matrix, books, n_recs = 10):\n",
    "    \"\"\"\n",
    "    Generate recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    recs_id = np.argsort(pred_matrix[user_index,:].tolist()[0])[::-1][0:n_recs]\n",
    "    recs = [books[i] for i in recs_id]    \n",
    "    print ('For user-{}, recommendations are: \\n'.format(user_index))\n",
    "    \n",
    "    return recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrix-Factorization Based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_svd(train, n_compo = 10, random_state = 11):\n",
    "    \"\"\"\n",
    "    Fit SVD to user-item-matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    tsvd = TruncatedSVD(n_components = n_compo, random_state = random_state)\n",
    "    tsvd.fit(train)\n",
    "    \n",
    "    return tsvd\n",
    "\n",
    "def predict_train(tsvd_obj, train):\n",
    "    \"\"\"\n",
    "    Predict ratings on training-data\n",
    "    \"\"\"\n",
    "    \n",
    "    train_predictions = np.dot(tsvd_obj.transform(train), tsvd_obj.components_)\n",
    "    \n",
    "    return train_predictions\n",
    "\n",
    "def predict_test(tsvd_obj, test):\n",
    "    \"\"\"\n",
    "    Predict ratings on test-data\n",
    "    \"\"\"\n",
    "    \n",
    "    test_predictions = np.dot(tsvd_obj.transform(test), tsvd_obj.components_)\n",
    "        \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Testing new user\n",
    "# new_user_1 = np.zeros((1, user_item_df.shape[1]))\n",
    "# new_user_1[0, 1] = 5\n",
    "# pred_user_1 = new_user_1.dot(sim_mat)/np.reshape(np.abs(sim_mat).sum(axis = 1), (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 \n",
    "\n",
    "## Execute Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (21532, 258)\n",
      "time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "amazon_file_loc = 'E://MRP//0102/Books.csv'\n",
    "\n",
    "col_names = ['productID', 'title', 'price', 'userID', 'profileName', 'helpfulness', 'score', 'time', 'summary', 'text']\n",
    "books_amazon = load_data(path = amazon_file_loc, cols = col_names)\n",
    "\n",
    "book_names = get_unique_books(path = amazon_file_loc)\n",
    "\n",
    "goodreads_file_loc = 'E://MRP//0331//Book-Recommender-System//data//books_df.csv'\n",
    "books_goodreads = load_goodreads_data(path = goodreads_file_loc)    \n",
    "\n",
    "common_book_dict = get_matching_books(amazon_books = book_names, goodreads_books = books_goodreads['Books'])\n",
    "\n",
    "list_df = subset_amazon_data(path = amazon_file_loc)        \n",
    "\n",
    "books_amazon_whole = extract_info(data = list_df)\n",
    "books_amazon_whole.head()\n",
    "\n",
    "user_item_df = create_user_item_df(ratings_df = books_amazon_whole)\n",
    "\n",
    "# List of books\n",
    "new_book_names = user_item_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-by-Item - Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4500000000000002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "user_item_df, test = generate_train_test_data(books_ratings = books_amazon_whole, user_item_mat = user_item_df)\n",
    "sim_mat = calculate_similarity(user_item_ratings = np.matrix(user_item_df))\n",
    "pred_ratings = predict_ratings_train(user_item_ratings = np.matrix(user_item_df), similarity_mat = sim_mat)\n",
    "pred_test = predict_ratings_test(test_data = test, similarity_mat = sim_mat)\n",
    "calculate_error(test_data = test, pred_data = pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Factorization - Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = fit_svd(train = user_item_df, n_compo = 25)\n",
    "predict_ratings_train = predict_train(tsvd_obj = tsvd, train = user_item_df)\n",
    "predict_ratings_test = predict_test(tsvd_obj = tsvd, test = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user-11, recommendations are: \n",
      "\n",
      "Item-by-item CF:  ['Mr. Timothy ', 'Exposure', 'Lioness', 'Doohickey ', 'Not the End of the World', 'Thicker Than Water', 'Sacrifice', 'Puppet', 'Nobody True', 'Blindsided']\n",
      "\n",
      "\n",
      "For user-11, recommendations are: \n",
      "\n",
      "Matrix-Factorization:  ['The Jester', 'Pompeii', 'Motherless Brooklyn', 'The Last Juror', 'Carrie', 'Just One Look', 'Rebecca', 'Tell No One', 'Me Talk Pretty One Day', 'Night Watch']\n",
      "time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "user_number = 11\n",
    "\n",
    "print ('Item-by-item CF: ', \n",
    "       get_recommendations(user_index = user_number, pred_matrix = pred_test, books = new_book_names, n_recs = 10))\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "print ('Matrix-Factorization: ', \n",
    "       get_recommendations(user_index = user_number, pred_matrix = np.matrix(predict_ratings_test), \n",
    "                           books = new_book_names, n_recs = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old-code (kept here just for my reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For EDA\n",
    "# amazon_rating = pd.DataFrame(books_amazon_whole.groupby(by = 'BookTitle').mean()).reset_index(drop = False)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(figsize= (14,7), ncols = 2, sharey = True)\n",
    "# ax1.hist(amazon_rating['Score'], bins = [1,2,3,4,5], normed = True)\n",
    "# ax1.set_xlabel('Average Rating', fontsize = 15)\n",
    "# ax1.set_ylabel('Frequency (Normalized)', fontsize = 15)\n",
    "# ax1.set_title('Distribution of Average Rating \\n (Amazon)', fontsize = 20)\n",
    "\n",
    "# ax2.hist(books_goodreads['Avg_Rating'], bins = [1,2,3,4,5], normed = True)\n",
    "# ax2.set_xlabel('Average Rating', fontsize = 15)\n",
    "# ax2.set_title('Distribution of Average Rating \\n (Goodreads)', fontsize = 20)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# user_data = [np.count_nonzero(user_item_df.iloc[i,:]) for i in range(user_item_df.shape[0])]\n",
    "# book_data = [np.count_nonzero(user_item_df.iloc[:,i]) for i in range(user_item_df.shape[1])]\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (14,6))\n",
    "# ax1.hist(user_data, bins = range(0,15,1))\n",
    "# ax2.hist(book_data, bins = range(0, 3000,50))\n",
    "\n",
    "# ax1.set_title('Distribution of User-Ratings', fontsize = 15)\n",
    "# ax2.set_title('Distribution of Book-Ratings', fontsize = 15)\n",
    "\n",
    "# ax1.set_xlabel('#Books rated by user', fontsize = 15)\n",
    "# ax2.set_xlabel('#Ratings each book received', fontsize = 15)\n",
    "\n",
    "# ax1.set_ylabel('#Users', fontsize = 15)\n",
    "# ax2.set_ylabel('#Books', fontsize = 15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_rmse(rmse_list, n_users = 500):\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize = (14,8))\n",
    "#     ax.plot(rmse_list[0:n_users])\n",
    "#     ax.axhline(y = np.mean(rmse_list), label = 'Avg. RMSE: {}'.format(round(np.mean(rmse_list), 3)), \n",
    "#                color = 'r', linestyle = 'dashed')\n",
    "#     ax.set_ylabel('RMSE', fontsize = 15)\n",
    "#     ax.set_xlabel('UserId', fontsize = 15)\n",
    "#     ax.set_title('RMSE for each user', fontsize = 20)\n",
    "#     ax.legend()\n",
    "#     plt.show()    \n",
    "    \n",
    "#     return None\n",
    "\n",
    "# rmse_train = np.sqrt(np.mean(np.array((np.matrix(user_item_df) - predict_ratings_train))**2, axis = 1))\n",
    "# plot_rmse(rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19 ms\n"
     ]
    }
   ],
   "source": [
    "# def get_recommended_books(user_id, books_list, latent_ratings, ui_mat, top_n = 15):\n",
    "    \n",
    "#     ## Get recommendations for a given user:\n",
    "#     ind_top_rated_books = np.argsort(latent_ratings.iloc[user_id])[::-1][0:top_n]\n",
    "#     recommended_books = [books_list[ind] for ind in ind_top_rated_books]    \n",
    "#     recommendation_df = pd.DataFrame({'UserID': user_id, 'BookID': ind_top_rated_books, \n",
    "#                                      'Recommended_Books': recommended_books})\n",
    "    \n",
    "#     ## Get actual books that the user rated:\n",
    "#     user_rated_books = ui_mat[user_id,:].toarray()\n",
    "#     rated_books_ind = np.argwhere(user_rated_books != 0)[:,1]\n",
    "#     rated_books = [books_list[ind] for ind in rated_books_ind]\n",
    "#     user_rated_books_df = pd.DataFrame({'BookID': rated_books_ind, 'RatedBooks': rated_books, 'UserID': user_id})\n",
    "    \n",
    "#     return user_rated_books_df, recommendation_df\n",
    "\n",
    "# ## Try: 211\n",
    "# user_rated_books, recommended_books = get_recommended_books(user_id = 211, books_list = new_book_names, \n",
    "#                                       latent_ratings = predict_ratings, ui_mat = user_item_mat)\n",
    "\n",
    "# rated_books, recommended_books = get_recommended_books(user_id = 100, books_list = new_book_names, \n",
    "#                                       latent_ratings = pd.DataFrame(predict_ratings_train), ui_mat = train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
